# Fix : Augmentation max_tokens pour √©pisodes longs

**Date :** 26 d√©cembre 2024
**Issues :** #92, #93
**Branche :** `92-bug-generation-resume-episode-du-29-mars-2020-echoue`
**Fichier modifi√© :** `ui/pages/4_avis_critiques.py`

## Probl√®me identifi√©

### Sympt√¥mes
La g√©n√©ration de r√©sum√©s d'avis critiques √©chouait sur certains √©pisodes longs avec le message :
```
‚ö†Ô∏è La r√©ponse de l'IA semble tronqu√©e (trop courte ou se termine brutalement)
üìä Longueur de la r√©ponse: 137 caract√®res
```

### √âpisodes affect√©s
- **29 mars 2020** : "Albert Camus, Cristina Comencini, Stephen King... Des livres en temps de confinement" (issue #92)
- **17 nov. 2019** : √âpisode long (issue #93)

Ces √©pisodes ont des transcriptions tr√®s longues (~3240 secondes = 54 minutes, ~16000 mots).

### Cause racine
**Limite de tokens insuffisante pour la r√©ponse.**

Le param√®tre `max_tokens=4000` √©tait trop faible pour g√©n√©rer des tableaux complets avec :
- 5+ livres √† analyser
- Avis d√©taill√©s de 4-5 critiques par livre
- Tableaux markdown format√©s avec HTML

Pour les √©pisodes tr√®s longs, m√™me si GPT-4o a une limite de contexte de 128K tokens :
- Le prompt + la longue transcription consomment beaucoup de tokens en **entr√©e**
- Il reste peu de marge pour une **sortie** d√©taill√©e
- La g√©n√©ration s'arr√™tait brutalement au milieu d'un tableau

## Solution impl√©ment√©e

### Modification du code
**Fichier :** `ui/pages/4_avis_critiques.py` (ligne 992)

**Avant :**
```python
response = model.complete(
    prompt,
    max_tokens=4000,  # Augmenter significativement la limite pour des r√©sum√©s d√©taill√©s
    temperature=0.1,
)
```

**Apr√®s :**
```python
response = model.complete(
    prompt,
    max_tokens=8000,  # Augment√© √† 8000 pour g√©rer les √©pisodes longs avec beaucoup de livres (fix #92)
    temperature=0.1,
)
```

### Pourquoi 8000 ?
- **4000 tokens** = ~3000 mots = insuffisant pour 5 livres avec avis d√©taill√©s
- **8000 tokens** = ~6000 mots = suffisant pour :
  - Tableau 1 : Livres du programme (5+ livres √ó 3-4 avis d√©taill√©s)
  - Tableau 2 : Coups de c≈ìur personnels
  - Formatage HTML pour les couleurs
  - Marge de s√©curit√©

### Impact
‚úÖ **Pas d'augmentation des co√ªts** : `max_tokens` est une limite maximale, pas un quota obligatoire. Les √©pisodes courts continueront √† g√©n√©rer des r√©ponses courtes.

## Tests effectu√©s

### Tests utilisateur (validation compl√®te)
1. ‚úÖ **√âpisode 29 mars 2020** : G√©n√©ration compl√®te r√©ussie
2. ‚úÖ **√âpisode 17 nov. 2019** : G√©n√©ration compl√®te r√©ussie (issue #93)
3. ‚úÖ **√âpisodes courts** : Fonctionnent toujours correctement
4. ‚úÖ **√âpisodes moyens** : Aucune r√©gression

### Tests automatis√©s
```bash
PYTHONPATH=/workspaces/lmelp/src python -m pytest tests/unit/
# R√©sultat : 249 passed in 1.27s
```

## Diff√©rence avec l'issue #90

### Issue #90 (PR #91)
**Probl√®me :** Prompt insuffisant pour ignorer le "courrier de la semaine"
**Solution :** Am√©lioration du prompt avec instructions explicites

### Issues #92 & #93 (cette PR)
**Probl√®me :** Limite de tokens insuffisante pour r√©ponses longues
**Solution :** Augmentation de `max_tokens` de 4000 √† 8000

Ces deux fixes sont **compl√©mentaires** :
- #91 am√©liore la **qualit√©** du prompt (quoi analyser)
- #92/#93 augmentent la **capacit√©** de r√©ponse (combien g√©n√©rer)

## Points cl√©s √† retenir

### 1. Red√©marrage Streamlit obligatoire
‚ö†Ô∏è **Important :** Un simple rafra√Æchissement du navigateur (F5) ne suffit PAS pour recharger le code Python modifi√© dans Streamlit.

**Proc√©dure correcte :**
```bash
# Dans le terminal o√π tourne Streamlit
Ctrl+C
./ui/lmelp_ui.sh
```

### 2. Diff√©rence entre tokens d'entr√©e et de sortie
- **Tokens d'entr√©e** : Prompt + transcription (non modifiable pour un √©pisode donn√©)
- **Tokens de sortie** : R√©ponse g√©n√©r√©e (contr√¥l√© par `max_tokens`)
- **Limite totale GPT-4o** : 128K tokens (entr√©e + sortie)

Pour les tr√®s longs √©pisodes :
- Entr√©e : ~50K-80K tokens (transcription longue)
- Sortie max possible : ~40K-70K tokens
- Notre limite : 8K tokens (largement suffisant, √©conomique)

### 3. D√©tection de troncature
Le code a d√©j√† une d√©tection robuste de r√©ponses tronqu√©es :
```python
if (
    len(response_text) < 300
    or response_text.endswith("**")
    or response_text.endswith("‚Üí")
):
    st.error("‚ö†Ô∏è La r√©ponse de l'IA semble tronqu√©e")
    return "R√©ponse de l'IA tronqu√©e. Veuillez r√©essayer."
```

Cette d√©tection a permis d'identifier rapidement le probl√®me.

## Fichiers modifi√©s

```
ui/pages/4_avis_critiques.py    | 2 +-
1 file changed, 1 insertion(+), 1 deletion(-)
```

## Commandes utiles

```bash
# V√©rifier les √©pisodes longs dans la base
db.episodes.find({duree: {$gte: 3000}}).sort({duree: -1}).limit(10)

# Tester la g√©n√©ration pour un √©pisode sp√©cifique
# Via l'interface Streamlit : Pages > üìù Avis Critiques > S√©lectionner √©pisode > G√©n√©rer

# Lancer les tests unitaires
PYTHONPATH=/workspaces/lmelp/src python -m pytest tests/unit/ -v
```

## Documentation associ√©e

- **Issue #92 :** https://github.com/castorfou/lmelp/issues/92
- **Issue #93 :** https://github.com/castorfou/lmelp/issues/93
- **PR #91 (fix courrier) :** https://github.com/castorfou/lmelp/pull/91
- **M√©moire pr√©c√©dente :** `251224-0916-fix-resume-avis-critiques-courrier.md`

## Le√ßons apprises

1. **Distinguer les probl√®mes de prompt vs limites techniques**
   - Probl√®me de prompt ‚Üí Am√©liorer les instructions
   - Probl√®me de tokens ‚Üí Ajuster les limites

2. **Tester avec des cas extr√™mes**
   - √âpisodes tr√®s longs (>50 min)
   - √âpisodes avec beaucoup de livres (5+)
   - √âpisodes avec "courrier de la semaine" long

3. **Le cache Streamlit est agressif**
   - Toujours red√©marrer l'app apr√®s modification Python
   - Ne pas se fier au rafra√Æchissement navigateur

4. **max_tokens est une limite max, pas un quota**
   - Augmenter max_tokens n'augmente pas les co√ªts si pas utilis√©
   - GPT-4o s'arr√™te naturellement quand la r√©ponse est compl√®te
