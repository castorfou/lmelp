# AUTOGENERATED! DO NOT EDIT! File to edit: avis_search.ipynb.

# %% auto 0
__all__ = [
    "AvisSearchEngine",
    "AutocompleteResult",
    "EpisodeAvis",
]

# %% avis_search.ipynb 1
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from bson import ObjectId
import logging
from datetime import datetime

# Fuzzy search
from thefuzz import fuzz, process

# Streamlit cache (si disponible)
try:
    import streamlit as st

    HAS_STREAMLIT = True
except ImportError:
    HAS_STREAMLIT = False

    # Mock du décorateur pour les tests
    def st_cache_data(func):
        return func

    st = type("MockStreamlit", (), {"cache_data": st_cache_data})()

# MongoDB
from mongo_episode_livre import EpisodeLivre
from mongo import get_collection
from config import get_DB_VARS

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# %% avis_search.ipynb 2
@dataclass
class AutocompleteResult:
    """Structure de données pour un résultat d'autocomplétion"""

    display_text: str  # Format "Auteur - Titre"
    auteur_nom: str
    livre_titre: str
    livre_oid: Optional[ObjectId] = None
    auteur_oid: Optional[ObjectId] = None
    nb_episodes: int = 0
    note_moyenne_globale: Optional[float] = None
    dernier_episode: Optional[str] = None
    score_fuzzy: int = 0  # Score de correspondance fuzzy


@dataclass
class EpisodeAvis:
    """Structure de données pour un épisode avec avis"""

    episode_oid: ObjectId
    episode_title: str
    episode_date: str
    note_moyenne: Optional[float] = None
    nb_critiques: Optional[int] = None
    coup_de_coeur: Optional[str] = None
    chef_doeuvre: Optional[str] = None
    avis_details: Optional[str] = None


# %% avis_search.ipynb 3
class AvisSearchEngine:
    """
    Moteur de recherche pour l'autocomplétion de livres/auteurs dans les avis critiques.

    Fonctionnalités :
    - Recherche combinée auteur + titre de livre
    - Recherche fuzzy pour correspondances approximatives
    - Cache Streamlit pour optimiser les performances
    - Format de suggestions : "Auteur - Titre"
    """

    def __init__(self, min_chars: int = 3, fuzzy_threshold: int = 70):
        """
        Initialise le moteur de recherche.

        Args:
            min_chars (int): Nombre minimum de caractères pour déclencher la recherche
            fuzzy_threshold (int): Score minimum pour les correspondances fuzzy
        """
        self.min_chars = min_chars
        self.fuzzy_threshold = fuzzy_threshold
        logger.info(
            f"AvisSearchEngine initialisé (min_chars={min_chars}, fuzzy_threshold={fuzzy_threshold})"
        )

    def search_combined(self, query: str, limit: int = 10) -> List[AutocompleteResult]:
        """
        Recherche combinée dans auteurs ET titres de livres.

        Args:
            query (str): Terme de recherche
            limit (int): Nombre maximum de résultats

        Returns:
            List[AutocompleteResult]: Résultats de recherche triés par pertinence
        """
        if len(query) < self.min_chars:
            return []

        logger.debug(f"Recherche combinée pour: '{query}'")

        # Recherche directe MongoDB d'abord
        direct_results = AvisSearchEngine._search_direct(query, limit)

        # Si peu de résultats, faire une recherche fuzzy
        if len(direct_results) < limit:
            fuzzy_results = self._search_fuzzy(query, limit - len(direct_results))

            # Combiner et dédupliquer
            all_results = direct_results + fuzzy_results
            seen = set()
            unique_results = []

            for result in all_results:
                key = (result.auteur_nom, result.livre_titre)
                if key not in seen:
                    seen.add(key)
                    unique_results.append(result)

            return unique_results[:limit]

        return direct_results

    @st.cache_data
    def _search_direct(_query: str, _limit: int) -> List[AutocompleteResult]:
        """
        Recherche directe dans MongoDB avec cache Streamlit.
        Note: Méthode statique pour compatibilité cache Streamlit.
        """
        return AvisSearchEngine._perform_direct_search(_query, _limit)

    @staticmethod
    def _perform_direct_search(query: str, limit: int) -> List[AutocompleteResult]:
        """
        Effectue une recherche directe dans la collection episode_livres.
        """
        try:
            # Utiliser la méthode de recherche de EpisodeLivre
            raw_results = EpisodeLivre.search_books_by_text(query, limit)

            results = []
            for raw in raw_results:
                result = AutocompleteResult(
                    display_text=f"{raw['auteur_nom']} - {raw['livre_titre']}",
                    auteur_nom=raw["auteur_nom"],
                    livre_titre=raw["livre_titre"],
                    livre_oid=raw.get("livre_oid"),
                    auteur_oid=raw.get("auteur_oid"),
                    nb_episodes=raw.get("nb_episodes", 0),
                    note_moyenne_globale=raw.get("note_moyenne_globale"),
                    dernier_episode=raw.get("dernier_episode"),
                    score_fuzzy=100,  # Score parfait pour recherche directe
                )
                results.append(result)

            logger.debug(f"Recherche directe '{query}': {len(results)} résultats")
            return results

        except Exception as e:
            logger.error(f"Erreur recherche directe: {e}")
            return []

    def _search_fuzzy(self, query: str, limit: int) -> List[AutocompleteResult]:
        """
        Recherche fuzzy sur tous les livres disponibles.
        """
        try:
            # Récupérer tous les livres uniques depuis la collection
            all_books = AvisSearchEngine._get_all_books_cached()

            if not all_books:
                return []

            # Préparer les textes pour la recherche fuzzy
            search_texts = []
            book_data = []

            for book in all_books:
                # Textes de recherche : auteur, titre, et combiné
                auteur = book["auteur_nom"]
                titre = book["livre_titre"]
                combined = f"{auteur} {titre}"

                search_texts.extend([auteur, titre, combined])
                book_data.extend(
                    [book, book, book]
                )  # Référence pour retrouver le livre

            # Recherche fuzzy avec thefuzz
            fuzzy_matches = process.extract(
                query,
                search_texts,
                scorer=fuzz.token_set_ratio,
                limit=limit * 3,  # Plus de résultats pour filtrer
            )

            # Convertir en AutocompleteResult et filtrer par score
            results = []
            seen_books = set()

            for match_text, score in fuzzy_matches:
                if score >= self.fuzzy_threshold:
                    # Retrouver le livre correspondant
                    for i, text in enumerate(search_texts):
                        if text == match_text:
                            book = book_data[i]
                            book_key = (book["auteur_nom"], book["livre_titre"])

                            if book_key not in seen_books:
                                seen_books.add(book_key)
                                result = AutocompleteResult(
                                    display_text=f"{book['auteur_nom']} - {book['livre_titre']}",
                                    auteur_nom=book["auteur_nom"],
                                    livre_titre=book["livre_titre"],
                                    livre_oid=book.get("livre_oid"),
                                    auteur_oid=book.get("auteur_oid"),
                                    nb_episodes=book.get("nb_episodes", 0),
                                    note_moyenne_globale=book.get(
                                        "note_moyenne_globale"
                                    ),
                                    dernier_episode=book.get("dernier_episode"),
                                    score_fuzzy=score,
                                )
                                results.append(result)
                            break

            # Trier par score fuzzy décroissant
            results.sort(key=lambda x: x.score_fuzzy, reverse=True)

            logger.debug(f"Recherche fuzzy '{query}': {len(results)} résultats")
            return results[:limit]

        except Exception as e:
            logger.error(f"Erreur recherche fuzzy: {e}")
            return []

    @st.cache_data
    def _get_all_books_cached() -> List[Dict[str, Any]]:
        """
        Récupère tous les livres avec cache Streamlit.
        """
        return AvisSearchEngine._get_all_books()

    @staticmethod
    def _get_all_books() -> List[Dict[str, Any]]:
        """
        Récupère tous les livres uniques de la collection episode_livres.
        """
        try:
            # Utiliser une agrégation MongoDB pour récupérer tous les livres uniques
            DB_HOST, DB_NAME, _ = get_DB_VARS()
            collection = get_collection(
                target_db=DB_HOST, client_name=DB_NAME, collection_name="episode_livres"
            )

            pipeline = [
                {
                    "$group": {
                        "_id": {
                            "livre_titre": "$livre_titre",
                            "auteur_nom": "$auteur_nom",
                        },
                        "livre_oid": {"$first": "$livre_oid"},
                        "auteur_oid": {"$first": "$auteur_oid"},
                        "editeur_nom": {"$first": "$editeur_nom"},
                        "nb_episodes": {"$sum": 1},
                        "note_moyenne_globale": {"$avg": "$note_moyenne"},
                        "dernier_episode": {"$max": "$episode_date"},
                    }
                },
                {
                    "$project": {
                        "auteur_nom": "$_id.auteur_nom",
                        "livre_titre": "$_id.livre_titre",
                        "livre_oid": 1,
                        "auteur_oid": 1,
                        "editeur_nom": 1,
                        "nb_episodes": 1,
                        "note_moyenne_globale": 1,
                        "dernier_episode": 1,
                        "_id": 0,
                    }
                },
            ]

            results = list(collection.aggregate(pipeline))
            logger.info(f"Cache tous livres: {len(results)} livres uniques")
            return results

        except Exception as e:
            logger.error(f"Erreur récupération tous livres: {e}")
            return []

    # %% avis_search.ipynb 4
    def get_book_episodes(
        self,
        livre_oid: ObjectId = None,
        auteur_nom: str = None,
        livre_titre: str = None,
        sort_by_date: bool = True,
    ) -> List[EpisodeAvis]:
        """
        Récupère tous les épisodes où un livre a été discuté.

        Args:
            livre_oid (ObjectId, optional): ObjectId du livre
            auteur_nom (str, optional): Nom de l'auteur (si livre_oid non disponible)
            livre_titre (str, optional): Titre du livre (si livre_oid non disponible)
            sort_by_date (bool): Trier par date décroissante

        Returns:
            List[EpisodeAvis]: Liste des épisodes chronologiques
        """
        try:
            if livre_oid:
                # Recherche par ObjectId (plus efficace)
                episode_livres = EpisodeLivre.find_by_livre(livre_oid, sort_by_date)
            elif auteur_nom and livre_titre:
                # Recherche par nom/titre
                episode_livres = self._find_episodes_by_names(
                    auteur_nom, livre_titre, sort_by_date
                )
            else:
                logger.warning("get_book_episodes: paramètres insuffisants")
                return []

            # Convertir en EpisodeAvis
            results = []
            for ep_livre in episode_livres:
                episode_avis = EpisodeAvis(
                    episode_oid=ep_livre.episode_oid,
                    episode_title=ep_livre.episode_title,
                    episode_date=ep_livre.episode_date,
                    note_moyenne=ep_livre.note_moyenne,
                    nb_critiques=ep_livre.nb_critiques,
                    coup_de_coeur=ep_livre.coup_de_coeur,
                    chef_doeuvre=ep_livre.chef_doeuvre,
                    avis_details=ep_livre.avis_details,
                )
                results.append(episode_avis)

            logger.info(f"Trouvé {len(results)} épisodes pour le livre")
            return results

        except Exception as e:
            logger.error(f"Erreur get_book_episodes: {e}")
            return []

    def _find_episodes_by_names(
        self, auteur_nom: str, livre_titre: str, sort_by_date: bool = True
    ) -> List[EpisodeLivre]:
        """
        Trouve les épisodes par nom d'auteur et titre de livre.
        """
        try:
            DB_HOST, DB_NAME, _ = get_DB_VARS()
            collection = get_collection(
                target_db=DB_HOST, client_name=DB_NAME, collection_name="episode_livres"
            )

            query = {"auteur_nom": auteur_nom, "livre_titre": livre_titre}

            cursor = collection.find(query)
            if sort_by_date:
                cursor = cursor.sort("episode_date", -1)

            results = []
            for doc in cursor:
                episode_livre = EpisodeLivre.from_oid(doc["_id"])
                if episode_livre:
                    results.append(episode_livre)

            return results

        except Exception as e:
            logger.error(f"Erreur find_episodes_by_names: {e}")
            return []

    def fuzzy_search(self, query: str, limit: int = 5) -> List[Tuple[str, str]]:
        """
        Recherche fuzzy simple retournant des tuples (auteur, titre).

        Args:
            query (str): Terme de recherche
            limit (int): Nombre maximum de résultats

        Returns:
            List[Tuple[str, str]]: Liste de tuples (auteur, titre)
        """
        results = self.search_combined(query, limit)
        return [(r.auteur_nom, r.livre_titre) for r in results]

    def format_suggestion(self, auteur: str, titre: str) -> str:
        """
        Formate une suggestion au standard "Auteur - Titre".

        Args:
            auteur (str): Nom de l'auteur
            titre (str): Titre du livre

        Returns:
            str: Suggestion formatée
        """
        return f"{auteur} - {titre}"

    def parse_selected_suggestion(self, suggestion: str) -> Optional[Tuple[str, str]]:
        """
        Parse une suggestion sélectionnée pour extraire auteur et titre.

        Args:
            suggestion (str): Suggestion au format "Auteur - Titre"

        Returns:
            Optional[Tuple[str, str]]: (auteur, titre) ou None si parsing échoue
        """
        try:
            if " - " in suggestion:
                parts = suggestion.split(" - ", 1)
                if len(parts) == 2:
                    return (parts[0].strip(), parts[1].strip())
            logger.warning(f"Impossible de parser la suggestion: '{suggestion}'")
            return None
        except Exception as e:
            logger.error(f"Erreur parsing suggestion: {e}")
            return None


# %% avis_search.ipynb 5
def test_search_engine():
    """Fonction de test pour valider le moteur de recherche"""

    print("=== TEST MOTEUR DE RECHERCHE ===")

    # Initialiser le moteur
    engine = AvisSearchEngine(min_chars=3, fuzzy_threshold=70)

    # Test de formatage
    suggestion = engine.format_suggestion(
        "Mario Vargas Llosa", "Je vous dédie mon silence"
    )
    print(f"Format suggestion: {suggestion}")

    # Test de parsing
    parsed = engine.parse_selected_suggestion(suggestion)
    print(f"Parsed: {parsed}")

    # Test avec des données fictives (sans base de données)
    print(f"Moteur initialisé avec succès")
    print(f"Min chars: {engine.min_chars}")
    print(f"Fuzzy threshold: {engine.fuzzy_threshold}")

    return True


if __name__ == "__main__":
    test_search_engine()
