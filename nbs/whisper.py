# AUTOGENERATED! DO NOT EDIT! File to edit: 09 whisper mp3.ipynb.

# %% auto 0
__all__ = [
    "AUDIO_PATH",
    "list_mp3_files",
    "list_audio_files",
    "extract_whisper",
    "extract_whisper_long",
    "store_whisper_in_db",
]

# %% 09 whisper mp3.ipynb 2
AUDIO_PATH = "audios"

# %% 09 whisper mp3.ipynb 4
from mongo_episode import get_audio_path
import os, glob
from typing import List, Optional


def list_mp3_files(
    audio_path=AUDIO_PATH, sort_by_size: Optional[str] = None
) -> List[str]:
    """
    Liste tous les fichiers MP3 dans le répertoire spécifié.
    Args:
        audio_path (str): Le chemin du répertoire contenant les fichiers audio. Par défaut, utilise la constante AUDIO_PATH.
        sort_by_size (Optional[str]): 'asc' pour trier par taille croissante, 'desc' pour décroissante, None pour aucun tri.
    Returns:
        list: Une liste des chemins de fichiers MP3 trouvés, triés si demandé.
    """
    fullpath = get_audio_path(audio_path, year="")
    files = glob.glob(os.path.join(fullpath, "**/*.mp3"), recursive=True)

    if sort_by_size:
        order = sort_by_size.lower()
        if order not in ("asc", "desc"):
            raise ValueError("sort_by_size must be 'asc', 'desc' or None")
        files.sort(key=lambda p: os.path.getsize(p), reverse=(order == "desc"))

    return files


def list_audio_files(
    audio_path=AUDIO_PATH, sort_by_size: Optional[str] = None
) -> List[str]:
    """
    Liste tous les fichiers audio (MP3 et M4A) dans le répertoire spécifié.

    Args:
        audio_path (str): Le chemin du répertoire contenant les fichiers audio. Par défaut, utilise la constante AUDIO_PATH.
        sort_by_size (Optional[str]): 'asc' pour trier par taille croissante, 'desc' pour décroissante, None pour aucun tri.

    Returns:
        list: Une liste des chemins de fichiers audio (MP3 et M4A) trouvés, triés si demandé.
    """
    fullpath = get_audio_path(audio_path, year="")

    mp3_files = glob.glob(os.path.join(fullpath, "**/*.mp3"), recursive=True)
    m4a_files = glob.glob(os.path.join(fullpath, "**/*.m4a"), recursive=True)

    files = mp3_files + m4a_files

    if sort_by_size:
        order = sort_by_size.lower()
        if order not in ("asc", "desc"):
            raise ValueError("sort_by_size must be 'asc', 'desc' or None")
        files.sort(key=lambda p: os.path.getsize(p), reverse=(order == "desc"))

    return files


# %% 09 whisper mp3.ipynb 8
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline


def extract_whisper(audio_filename: str) -> str:
    """
    Extrait la transcription d'un fichier audio en utilisant le modèle Whisper.

    Args:
        audio_filename (str): Le chemin du fichier audio à transcrire.

    Returns:
        str: La transcription du fichier audio.
    """
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

    model_id = "openai/whisper-large-v3-turbo"

    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)

    processor = AutoProcessor.from_pretrained(model_id)

    generate_kwargs = {
        "language": "fr",
    }

    pipe = pipeline(
        "automatic-speech-recognition",
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        torch_dtype=torch_dtype,
        device=device,
        chunk_length_s=30,
        batch_size=16,  # batch size for inference - set based on your device
        generate_kwargs=generate_kwargs,
    )

    result = pipe(
        audio_filename,
        return_timestamps=True,
        ignore_warning=True,
    )

    return result["text"]


# %% 09 whisper mp3.ipynb 11
from pydub import AudioSegment
import tempfile
import os
import soundfile as sf
import torch


def extract_whisper_long(
    audio_filename: str, chunk_s: int = 30, overlap_s: int = 1
) -> str:
    """
    Splits long audio into overlapping chunks, preprocesses each chunk with the processor
    (using return_attention_mask) and runs model.generate for each chunk, then returns concatenated text.

    Important: ensure audio chunks are sampled at the feature extractor's expected rate (16kHz).
    This implementation exports each chunk to WAV at 16000 Hz to avoid the sampling-rate ValueError.
    """
    device = "cuda:0" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    model_id = "openai/whisper-large-v3-turbo"

    model = AutoModelForSpeechSeq2Seq.from_pretrained(
        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
    )
    model.to(device)
    processor = AutoProcessor.from_pretrained(model_id)

    generate_kwargs = {"language": "fr"}

    seg = AudioSegment.from_file(audio_filename)
    dur_ms = len(seg)
    step_ms = (chunk_s - overlap_s) * 1000
    chunk_ms = chunk_s * 1000
    texts = []

    for start in range(0, max(1, dur_ms), int(step_ms)):
        end = start + chunk_ms
        piece = seg[start:end]
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            tmp_path = tmp.name
            # Export the chunk as WAV forced to 16 kHz to match WhisperFeatureExtractor expectations
            # requires ffmpeg (used by pydub) available in the container.
            piece.export(tmp_path, format="wav", parameters=["-ar", "16000"])
        try:
            # read chunk as numpy array (float) + sr
            speech, sr = sf.read(tmp_path)
            if sr != processor.feature_extractor.sampling_rate:
                raise RuntimeError(
                    f"Unexpected sampling rate {sr}, expected {processor.feature_extractor.sampling_rate}"
                )

            # processor -> return_tensors="pt" and return_attention_mask to follow deprecation guidance
            inputs = processor(
                speech,
                sampling_rate=sr,
                return_tensors="pt",
                return_attention_mask=True,
            )
            # support both possible keys
            feat = inputs.get("input_features", inputs.get("input_values"))
            if feat is None:
                raise RuntimeError("Processor did not return input features")
            feat = feat.to(device).to(dtype=torch_dtype)

            # run generate on model
            generated = model.generate(feat, **generate_kwargs)
            decoded = processor.tokenizer.batch_decode(
                generated, skip_special_tokens=True
            )
            texts.append(decoded[0].strip() if decoded else "")
        finally:
            os.remove(tmp_path)
        if end >= dur_ms:
            break

    return " ".join(t for t in texts if t)


# %% 09 whisper mp3.ipynb 15
from bson import ObjectId
import pymongo


def store_whisper_in_db(
    whisper: str,
    collection: pymongo.collection.Collection,
    oid: str,
    force: bool = False,
    verbose: bool = False,
) -> bool:
    """
    Stocke la transcription Whisper dans la base de données.

    Args:
        whisper (str): La transcription du fichier audio.
        collection: La collection pymongo.
        oid (str): L'identifiant de l'épisode.
        force (bool, optional): Si True, écrase le Whisper existant. Par défaut, False.
        verbose (bool, optional): Si True, affiche des messages détaillés. Par défaut, False.

    Returns:
        bool: True si le Whisper a été stocké, False sinon.
    """
    # Récupération du document
    document_entry = collection.find_one({"_id": ObjectId(oid)})

    if document_entry is None:
        if verbose:
            print(f"Document avec l'oid {oid} non trouvé")
        return False

    if "whisper" in document_entry and not force:
        if verbose:
            print(
                f"Whisper déjà stocké pour l'oid {oid}, et on ne force pas le stockage"
            )
        return False
    else:
        document_entry["whisper"] = whisper
        collection.update_one({"_id": ObjectId(oid)}, {"$set": document_entry})
        if verbose:
            print(f"Whisper stocké pour l'oid {oid}")
        return True
