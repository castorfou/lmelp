{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def load_env():\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "def get_gemini_api_key():\n",
    "    load_env()\n",
    "    gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    return gemini_api_key\n",
    "\n",
    "\n",
    "def get_openai_api_key():\n",
    "    load_env()\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return openai_api_key\n",
    "\n",
    "\n",
    "def get_google_projectID():\n",
    "    load_env()\n",
    "    google_projectID = os.getenv(\"GOOGLE_PROJECT_ID\")\n",
    "    return google_projectID\n",
    "\n",
    "\n",
    "def get_google_auth_file():\n",
    "    load_env()\n",
    "    google_auth_file = os.getenv(\"GOOGLE_AUTH_FILE\")\n",
    "    return google_auth_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_key = get_gemini_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get gemini llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def get_gemini_llm(model=\"gemini-1.5-flash\"):\n",
    "    genai.configure(api_key=get_gemini_api_key())\n",
    "    llm = genai.GenerativeModel(model)\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) doesn't work in a single, unified way.  Instead, it encompasses a broad\n"
     ]
    }
   ],
   "source": [
    "llm = get_gemini_llm()\n",
    "response = llm.generate_content(\"Explain how AI works\")\n",
    "print(response.text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get gemini llamaindex llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "def get_gemini_llamaindex_llm(model=\"models/gemini-1.5-flash\"):\n",
    "    genai.configure(api_key=get_gemini_api_key())\n",
    "    llm = Gemini(model=model, api_key=get_gemini_api_key())\n",
    "    Settings.llm = llm\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A worn leather back, a canvas so deep,\n",
      "Holds secrets untold, while others do sleep.\n",
      "No ordinary pack, this one holds the key,\n",
      "To wonders unseen, for you and for me.\n",
      "\n",
      "A whisper of starlight, a shimmer of gold,\n",
      "A story unfolds, as its magic's unrolled.\n",
      "A compass that points to a sun-drenched lagoon,\n",
      "A map etched in starlight, beneath the pale moon.\n",
      "\n",
      "A vial of laughter, a tear that's a gem,\n",
      "A feather that floats, a forgotten diadem.\n",
      "A potion of courage, to conquer all fear,\n",
      "A songbird's sweet melody, always held near.\n",
      "\n",
      "It shrinks to a pebble, then bursts into size,\n",
      "A portal it opens, to fantastical skies.\n",
      "With a flick of the wrist, and a magical rhyme,\n",
      "It bends space and time, transcending all clime.\n",
      "\n",
      "So carry it lightly, this treasure you hold,\n",
      "Its stories unwritten, its wonders untold.\n",
      "For the magic backpack, a gift from above,\n",
      "Is a journey of wonder, a testament to love.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm = get_gemini_llamaindex_llm()\n",
    "resp = llm.complete(\"Write a poem about a magic backpack\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get vertex (gemini llamaindex) llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.vertex import Vertex\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "def get_vertex_llm(model=\"gemini-1.5-flash-001\"):\n",
    "\n",
    "    # Set up necessary variables\n",
    "    credentials = {\n",
    "        \"project_id\": get_google_projectID(),\n",
    "        \"api_key\": get_gemini_api_key(),\n",
    "    }\n",
    "\n",
    "    # filename = get_google_auth_file()\n",
    "    # credentials_service: service_account.Credentials = (\n",
    "    #     service_account.Credentials.from_service_account_file(filename)\n",
    "    # )\n",
    "\n",
    "    # Create an instance of the Vertex class\n",
    "    llm = Vertex(\n",
    "        model=model,\n",
    "        project=credentials[\"project_id\"],\n",
    "        credentials=credentials,\n",
    "        context_window=100000,\n",
    "    )\n",
    "    # llm = Vertex(\n",
    "    #     model=model,\n",
    "    #     project=credentials_service.project_id,\n",
    "    #     credentials=credentials_service,\n",
    "    #     context_window=100000,    )\n",
    "    Settings.llm = llm\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n",
      "Retrying llama_index.llms.vertex.utils.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised ServiceUnavailable: 503 Getting metadata from plugin failed with error: 'dict' object has no attribute 'before_request'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m get_vertex_llm()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a poem about a magic backpack\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:431\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    423\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    424\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m     },\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 431\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    434\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    435\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m    436\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m    437\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/llama_index/llms/vertex/base.py:279\u001b[0m, in \u001b[0;36mVertex.complete\u001b[0;34m(self, prompt, formatted, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miscode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count is not supported by the codey model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 279\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_gemini\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_gemini\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletionResponse(text\u001b[38;5;241m=\u001b[39mcompletion\u001b[38;5;241m.\u001b[39mtext, raw\u001b[38;5;241m=\u001b[39mcompletion\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/llama_index/llms/vertex/utils.py:113\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(client, prompt, max_retries, chat, stream, is_gemini, params, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mpredict(prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/tenacity/__init__.py:485\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[1;32m    484\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[0;32m--> 485\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[0;32m~/miniforge3/envs/gemini/lib/python3.11/site-packages/tenacity/nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(seconds)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = get_vertex_llm()\n",
    "\n",
    "llm.complete(\"Write a poem about a magic backpack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# py code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "\n",
    "nb_export(\"helper.ipynb\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some good names for a flower shop specializing in dried flowers, playing on different themes:\n",
      "\n",
      "**Evoking Timeless Beauty:**\n",
      "\n",
      "* Everlasting Bloom\n",
      "* The Dried Bouquet\n",
      "* Bloom & Preserve\n",
      "* Eternal Blooms\n",
      "* Whispers of Bloom\n",
      "* Timeworn Flowers\n",
      "* Dried & Divine\n",
      "* The Paper Petal\n",
      "* Timeless Botanicals\n",
      "* Dried Flower Dreams\n",
      "\n",
      "**Highlighting Uniqueness & Sustainability:**\n",
      "\n",
      "* The Botanical Revival\n",
      "* Bloom Again\n",
      "* The Eco Bloom\n",
      "* Earth & Petal\n",
      "* Wild & Dried\n",
      "* Rebloom\n",
      "* Rustic Blossoms\n",
      "* The Petal Collective\n",
      "* Naturally Dried\n",
      "* Sustainably Bloom\n",
      "\n",
      "**Short & Catchy:**\n",
      "\n",
      "* Bloom Bar\n",
      "* Petal & Co.\n",
      "* The Dried Florist\n",
      "* Poppy & Oak\n",
      "* Bloom Box\n",
      "* The Flower Press\n",
      "* Bloom & Leaf\n",
      "\n",
      "**Bonus:**\n",
      "\n",
      "* You can also incorporate your location or your own name into the name. For example, \"The Willow Creek Dried Florist\" or \"Amelia's Everlasting Blooms.\"\n",
      "\n",
      "**Tips for Choosing a Name:**\n",
      "\n",
      "* **Keep it memorable.**  Easy to pronounce and spell.\n",
      "* **Consider your target audience.** Who are you trying to reach?\n",
      "* **Check for availability.** Ensure the name isn't already taken.\n",
      "* **Make sure it reflects your brand.** What kind of vibe do you want to convey?\n",
      "* **Get feedback.** Ask friends and family for their opinions.\n",
      "\n",
      "I hope this helps you find the perfect name for your flower shop! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# TODO(developer): Update & uncomment line below\n",
    "# PROJECT_ID = \"your-project-id\"\n",
    "vertexai.init(project=\"lmelp-446815\", location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-001\")\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"What's a good name for a flower shop that specializes in selling bouquets of dried flowers?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
