{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "AUDIO_PATH = \"audios\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list mp3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from mongo_episode import get_audio_path\n",
    "import os, glob\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "def list_mp3_files(\n",
    "    audio_path=AUDIO_PATH, sort_by_size: Optional[str] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Liste tous les fichiers MP3 dans le répertoire spécifié.\n",
    "    Args:\n",
    "        audio_path (str): Le chemin du répertoire contenant les fichiers audio. Par défaut, utilise la constante AUDIO_PATH.\n",
    "        sort_by_size (Optional[str]): 'asc' pour trier par taille croissante, 'desc' pour décroissante, None pour aucun tri.\n",
    "    Returns:\n",
    "        list: Une liste des chemins de fichiers MP3 trouvés, triés si demandé.\n",
    "    \"\"\"\n",
    "    fullpath = get_audio_path(audio_path, year=\"\")\n",
    "    files = glob.glob(os.path.join(fullpath, \"**/*.mp3\"), recursive=True)\n",
    "\n",
    "    if sort_by_size:\n",
    "        order = sort_by_size.lower()\n",
    "        if order not in (\"asc\", \"desc\"):\n",
    "            raise ValueError(\"sort_by_size must be 'asc', 'desc' or None\")\n",
    "        files.sort(key=lambda p: os.path.getsize(p), reverse=(order == \"desc\"))\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def list_audio_files(\n",
    "    audio_path=AUDIO_PATH, sort_by_size: Optional[str] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Liste tous les fichiers audio (MP3 et M4A) dans le répertoire spécifié.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Le chemin du répertoire contenant les fichiers audio. Par défaut, utilise la constante AUDIO_PATH.\n",
    "        sort_by_size (Optional[str]): 'asc' pour trier par taille croissante, 'desc' pour décroissante, None pour aucun tri.\n",
    "\n",
    "    Returns:\n",
    "        list: Une liste des chemins de fichiers audio (MP3 et M4A) trouvés, triés si demandé.\n",
    "    \"\"\"\n",
    "    fullpath = get_audio_path(audio_path, year=\"\")\n",
    "\n",
    "    mp3_files = glob.glob(os.path.join(fullpath, \"**/*.mp3\"), recursive=True)\n",
    "    m4a_files = glob.glob(os.path.join(fullpath, \"**/*.m4a\"), recursive=True)\n",
    "\n",
    "    files = mp3_files + m4a_files\n",
    "\n",
    "    if sort_by_size:\n",
    "        order = sort_by_size.lower()\n",
    "        if order not in (\"asc\", \"desc\"):\n",
    "            raise ValueError(\"sort_by_size must be 'asc', 'desc' or None\")\n",
    "        files.sort(key=lambda p: os.path.getsize(p), reverse=(order == \"desc\"))\n",
    "\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspaces/lmelp/audios/2016/14007-27.11.2016-ITEMA_21148290-0.mp3',\n",
       " '/workspaces/lmelp/audios/2024/14007-10.11.2024-ITEMA_23920569-2024F4007S0315-22.mp3',\n",
       " '/workspaces/lmelp/audios/2025/14007-14.09.2025-ITEMA_24246092-2025F4007S0257-NET_MFI_1A3CF54C-C3D3-4ABF-805B-2B3A8E4ED735-22.mp3',\n",
       " '/workspaces/lmelp/audios/2024/14007-01.12.2024-ITEMA_23942372-2024F4007S0336-22.mp3',\n",
       " '/workspaces/lmelp/audios/2025/14007-07.09.2025-ITEMA_24238506-2025F4007S0250-NET_MFI_362EAAE4-F232-40E7-9F69-12467ED0D96A-22.mp3']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_mp3_files(sort_by_size=\"asc\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspaces/lmelp/audios/1969/14007-30.11.1969-ITEMA_23787926-2024F4007E0117-27.m4a',\n",
       " '/workspaces/lmelp/audios/1996/14007-08.12.1996-ITEMA_23787640-2024F4007E0090-27.m4a',\n",
       " '/workspaces/lmelp/audios/1960/14007-15.12.1960-ITEMA_23787917-2024F4007E0110-27.m4a',\n",
       " '/workspaces/lmelp/audios/1984/14007-11.11.1984-ITEMA_23787854-2024F4007E0102-27.m4a',\n",
       " '/workspaces/lmelp/audios/1992/14007-27.09.1992-ITEMA_23787897-2024F4007E0094-27.m4a']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_audio_files(sort_by_size=\"desc\")[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "\n",
    "def extract_whisper(audio_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrait la transcription d'un fichier audio en utilisant le modèle Whisper.\n",
    "\n",
    "    Args:\n",
    "        audio_filename (str): Le chemin du fichier audio à transcrire.\n",
    "\n",
    "    Returns:\n",
    "        str: La transcription du fichier audio.\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    generate_kwargs = {\n",
    "        \"language\": \"fr\",\n",
    "    }\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,  # batch size for inference - set based on your device\n",
    "        generate_kwargs=generate_kwargs,\n",
    "    )\n",
    "\n",
    "    result = pipe(\n",
    "        audio_filename,\n",
    "        return_timestamps=True,\n",
    "        ignore_warning=True,\n",
    "    )\n",
    "\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" France Inter C'était très bien accueilli aussi par la presse dès le début de la saison C'est le livre de Patrick Chamoiseau chez Gallimard, Texaco Ch\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petit_audio = list_audio_files(sort_by_size=\"asc\")[0]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"filename: {petit_audio} \")\n",
    "\n",
    "whisper = extract_whisper(petit_audio)\n",
    "\n",
    "whisper[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract whisper long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "\n",
    "def extract_whisper_long(\n",
    "    audio_filename: str, chunk_s: int = 30, overlap_s: int = 1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Splits long audio into overlapping chunks, preprocesses each chunk with the processor\n",
    "    (using return_attention_mask) and runs model.generate for each chunk, then returns concatenated text.\n",
    "\n",
    "    Important: ensure audio chunks are sampled at the feature extractor's expected rate (16kHz).\n",
    "    This implementation exports each chunk to WAV at 16000 Hz to avoid the sampling-rate ValueError.\n",
    "    \"\"\"\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    generate_kwargs = {\"language\": \"fr\"}\n",
    "\n",
    "    seg = AudioSegment.from_file(audio_filename)\n",
    "    dur_ms = len(seg)\n",
    "    step_ms = (chunk_s - overlap_s) * 1000\n",
    "    chunk_ms = chunk_s * 1000\n",
    "    texts = []\n",
    "\n",
    "    for start in range(0, max(1, dur_ms), int(step_ms)):\n",
    "        end = start + chunk_ms\n",
    "        piece = seg[start:end]\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
    "            tmp_path = tmp.name\n",
    "            # Export the chunk as WAV forced to 16 kHz to match WhisperFeatureExtractor expectations\n",
    "            # requires ffmpeg (used by pydub) available in the container.\n",
    "            piece.export(tmp_path, format=\"wav\", parameters=[\"-ar\", \"16000\"])\n",
    "        try:\n",
    "            # read chunk as numpy array (float) + sr\n",
    "            speech, sr = sf.read(tmp_path)\n",
    "            if sr != processor.feature_extractor.sampling_rate:\n",
    "                raise RuntimeError(\n",
    "                    f\"Unexpected sampling rate {sr}, expected {processor.feature_extractor.sampling_rate}\"\n",
    "                )\n",
    "\n",
    "            # processor -> return_tensors=\"pt\" and return_attention_mask to follow deprecation guidance\n",
    "            inputs = processor(\n",
    "                speech,\n",
    "                sampling_rate=sr,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "            # support both possible keys\n",
    "            feat = inputs.get(\"input_features\", inputs.get(\"input_values\"))\n",
    "            if feat is None:\n",
    "                raise RuntimeError(\"Processor did not return input features\")\n",
    "            feat = feat.to(device).to(dtype=torch_dtype)\n",
    "\n",
    "            # run generate on model\n",
    "            generated = model.generate(feat, **generate_kwargs)\n",
    "            decoded = processor.tokenizer.batch_decode(\n",
    "                generated, skip_special_tokens=True\n",
    "            )\n",
    "            texts.append(decoded[0].strip() if decoded else \"\")\n",
    "        finally:\n",
    "            os.remove(tmp_path)\n",
    "        if end >= dur_ms:\n",
    "            break\n",
    "\n",
    "    return \" \".join(t for t in texts if t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "filename: /workspaces/lmelp/audios/1992/14007-27.09.1992-ITEMA_23787897-2024F4007E0094-27.m4a \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "petit_audio = list_audio_files(sort_by_size=\"asc\")[0]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"filename: {petit_audio} \")\n",
    "\n",
    "whisper_long = extract_whisper_long(petit_audio)\n",
    "\n",
    "whisper_long[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"C'était très bien accueilli aussi par la presse dès le début de la saison, c'est le livre de Patrick Chamoiseau chez Gallimard, Texaco. Chamoiseau qui est un des jeunes écrivains antillais qui s'est fait connaître depuis quelques années avec de très beaux livres et qui là c'est vrai a les honneurs de toute la presse depuis quelques semaines. Alors qui a lu Texaco ici? Sophie Chérez, vous avez lu? Personne, comme tout le monde ne l'a parlé, personne. On ne l'est pas au programme. On ne l'est pas au programme, c'est comme à la rentrée scolaire. c'est comme à la rentrée scolaire non plus, Jean-Lidier? Moi je l'ai lu. Josiane? Oui, moi je l'ai lu, je l'ai défendu, je trouve ça très très beau. C'est un livre en effet un peu lourd comme Jean-François, je pense qu'il trouverait ça, c'est assez touffu, c'est un gros livre. Mais ce que j'aime beaucoup chez Chamoiseau, c'est la manière dont il utilise la langue française, c'est pas du créole, c'est un français qui l'a, comme dit Milan Kundera, chamoisisé. Et moi je trouve que cette manière d'essayer de raconter de raconter de l'histoire de sa culture, de son pays, l'histoire de la Martinique, de nous faire comprendre l'histoire de ces gens qui sont à la fois français et loin de nous, et de montrer que la langue française est une langue qui peut accueillir tout un tas de distorsions, tout un tas de nouveautés. Moi, je trouve ça très, très beau. Donc, en fait, l'année dernière, on a eu la confirmation et la révélation de Raphaël Confiant. Et cette année, c'est son ami et confrère Patrick Chamoiseau chez Gallimard. Moi, je préfère Chamoiseau. C'est moins dogmatique et plus poétique. Donc, Texaco de Patrick Chamoiseau chez Gallimard. Ça fait un peu Claude Meb, non? Je ne trouve pas.\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# store whisper in db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "from bson import ObjectId\n",
    "import pymongo\n",
    "\n",
    "\n",
    "def store_whisper_in_db(\n",
    "    whisper: str,\n",
    "    collection: pymongo.collection.Collection,\n",
    "    oid: str,\n",
    "    force: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Stocke la transcription Whisper dans la base de données.\n",
    "\n",
    "    Args:\n",
    "        whisper (str): La transcription du fichier audio.\n",
    "        collection: La collection pymongo.\n",
    "        oid (str): L'identifiant de l'épisode.\n",
    "        force (bool, optional): Si True, écrase le Whisper existant. Par défaut, False.\n",
    "        verbose (bool, optional): Si True, affiche des messages détaillés. Par défaut, False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si le Whisper a été stocké, False sinon.\n",
    "    \"\"\"\n",
    "    # Récupération du document\n",
    "    document_entry = collection.find_one({\"_id\": ObjectId(oid)})\n",
    "\n",
    "    if document_entry is None:\n",
    "        if verbose:\n",
    "            print(f\"Document avec l'oid {oid} non trouvé\")\n",
    "        return False\n",
    "\n",
    "    if \"whisper\" in document_entry and not force:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Whisper déjà stocké pour l'oid {oid}, et on ne force pas le stockage\"\n",
    "            )\n",
    "        return False\n",
    "    else:\n",
    "        document_entry[\"whisper\"] = whisper\n",
    "        collection.update_one({\"_id\": ObjectId(oid)}, {\"$set\": document_entry})\n",
    "        if verbose:\n",
    "            print(f\"Whisper stocké pour l'oid {oid}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper stocké pour l'oid 6773e32258fc5717f3516b98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import get_collection\n",
    "\n",
    "col = get_collection()\n",
    "oid = \"6773e32258fc5717f3516b98\"\n",
    "store_whisper_in_db(\"test whisper\", col, oid, force=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "\n",
    "nb_export(\"09 whisper mp3.ipynb\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
